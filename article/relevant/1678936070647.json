{"relevantArticles":[{"articleTitle":"Linux纯命令行配置clash，自由访问GPT api","articleAbstractText":"clash真是难用啊，文档也不好好写，Linux上配置却叫我浏览器访问设置节点。。。 搞了两天终于配置起来了，安装clash的教程基本都千篇一律吧，主要重点在如何命令行配置网络代理，没有一篇教程说明是如何命令行配置的。。。大家服务器都是桌面版的吗，头大 我服务器的操作系统是Ubuntu 安装clash 老三样，下载，解压，安装 # 下载clash，Github地址：https://github.com/Dreamacro/clash/releases # 两种方式： # 1.访问GitHub本地下载了上传到服务器，网络不好的建议此方式 # 2.wget 命令服务器下载 wget https://github.com/Dreamacro/clash/releases/download/v1.16.0/clash-linux-amd64-v1.16.0.gz # 解压 gzip -d clash-linux-amd64-v1.16.0.gz # 解压之后就是二进制执行文件了，不需要安装，修改一下权限，然后执行： # 修改权限 sudo chmod +x clash-linux-amd64-....","articleStatus":0,"articlePermalink":"/articles/2023/05/30/1685437715197.html","articleImg1URL":"https://b3logfile.com/bing/20181112.jpg?imageView2/1/w/960/h/540/interlace/1/q/100"},{"articleTitle":"【bigdata】3.idea连接虚拟机Hadoop集群","articleAbstractText":"1.windows配置Hadoop环境 1.Linux环境下解压hadoop-2.6.5的tar包 tar -zxvf hadoop-2.6.5.tar.gz，复制解压包到windows的任意目录下 2.下载winutils,copy相应版本bin目录下的winutils.exe和hadoop.dll到解压的Hadoop目录下 3.复制hadoop.dll到C:\\Windows\\System32目录下 4.配置环境变量 4.1配置hadoop_home  4.2Path路径添加hadoop_home/bin 2.maven项目配置Hadoop 1.pom文件 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;project&nbsp;xmlns=\"http://maven.apache.org/POM/4.0.0\" &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" &nbsp....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678937003866.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-17060812201894802840-cHOlGII.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【bigdata】Hadoop总结","articleAbstractText":"hadoop组成 hdfs架构 namenode：存储文件的元数据 datanode: 存储文件块数据 文件块大小，2以前64M,2之后是128M secondary namenode:监控hdfs状态的辅助后台程序，每隔一段时间获取元数据的快照 hdfs写数据流程  1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。 2）NameNode返回是否可以上传。 3）客户端请求第一个 Block上传到哪几个DataNode服务器上。 4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。 5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 6）dn1、dn2、dn3逐级应答客户端。 7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个pac....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678937719444.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-15822367608376695852-lN8cZwI.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【bigdata】hive总结","articleAbstractText":"hive本质：将hql转成mapreduce hive架构原理  1．用户接口：Client CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive） 2．元数据：Metastore 元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等； 默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore 3．Hadoop 使用HDFS进行存储，使用MapReduce进行计算。 4．驱动器：Driver （1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。 （2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。 （3）优化器（Query Optimizer）：对逻辑执行计划进行优化。 （4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/S....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678937928724.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-2478825779580391334-X3DtlWc.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【ML】win11+RTX3060搭建tf2.6深度学习环境","articleAbstractText":"win11+RTX3060搭建tf2.6深度学习环境 1.所需软件 cuda cudnn cudnn下载需要注册账号 anaconda tf2.6.2 2.安装cuda cuda简介： CUDA是NVIDIA发明的一种并行计算平台和编程模型。 它可以通过利用图形处理器(GPU)的能力来显著提高计算性能。 CUDA的开发有以下几个设计目标: 1.为标准编程语言(如C)提供一小组扩展，以实现并行算法的直接实现。 2.使用CUDA C/ c++，程序员可以专注于算法的并行化，而不是把时间花在算法的实现上。 3.支持异构计算，应用同时使用CPU和GPU。 应用程序的串行部分运行在CPU上，并行部分被卸载到GPU上， 这样CUDA就可以增量地应用到现有的应用程序上。 CPU和GPU被视为具有各自内存空间的独立设备。 这种配置还允许在CPU和GPU上进行同步计算，而无需争夺内存资源。 cuda支持的GPU有数百个内核，这些内核可以同时运行数千个计算线程。 这些核心拥有共享的资源，包括一个寄存器文件和一个共享的内存。 片上共享内存允许运行在这些核心上的并行任务共享数据，而无需通过系统内存总线发送数据....","articleStatus":0,"articlePermalink":"/articles/2023/03/22/1679472011598.html","articleImg1URL":"https://b3logfile.com/bing/20191112.jpg?imageView2/1/w/1280/h/720/interlace/1/q/100"}]}