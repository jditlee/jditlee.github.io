{"relevantArticles":[{"articleTitle":"【bigdata】Flink总结","articleAbstractText":"Flink 几个最基础的概念: Client、JobManager 和 TaskManager。Client 用来提交任务给 JobManager，JobManager 分发任务给 TaskManager 去执行，然后 TaskManager 会心跳的汇报任务状态 窗口： stream.timewindow:时间 stream.countwindow:计数 stream.window(SessionWindows.withGap(Time.minutes(5))：会话 Flink程序的基本构建块是流和转换。 一个程序的基本构成： l 获取execution environment l 加载/创建原始数据 l 指定这些数据的转化方法 l 指定计算结果的存放位置 l 触发程序执行 import&nbsp;org.apache.flink.api.common.functions.FlatMapFunction; import&nbsp;org.apache.flink.api.java.tuple.Tuple2; import&nbsp;org.apache.flink.streaming....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678938322285.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-3494035270768043777-E6F9Est.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【bigdata】hive总结","articleAbstractText":"hive本质：将hql转成mapreduce hive架构原理  1．用户接口：Client CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive） 2．元数据：Metastore 元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等； 默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore 3．Hadoop 使用HDFS进行存储，使用MapReduce进行计算。 4．驱动器：Driver （1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。 （2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。 （3）优化器（Query Optimizer）：对逻辑执行计划进行优化。 （4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/S....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678937928724.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-2478825779580391334-X3DtlWc.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【bigdata】Hadoop总结","articleAbstractText":"hadoop组成 hdfs架构 namenode：存储文件的元数据 datanode: 存储文件块数据 文件块大小，2以前64M,2之后是128M secondary namenode:监控hdfs状态的辅助后台程序，每隔一段时间获取元数据的快照 hdfs写数据流程  1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。 2）NameNode返回是否可以上传。 3）客户端请求第一个 Block上传到哪几个DataNode服务器上。 4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。 5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 6）dn1、dn2、dn3逐级应答客户端。 7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个pac....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678937719444.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-15822367608376695852-lN8cZwI.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【bigdata】3.idea连接虚拟机Hadoop集群","articleAbstractText":"1.windows配置Hadoop环境 1.Linux环境下解压hadoop-2.6.5的tar包 tar -zxvf hadoop-2.6.5.tar.gz，复制解压包到windows的任意目录下 2.下载winutils,copy相应版本bin目录下的winutils.exe和hadoop.dll到解压的Hadoop目录下 3.复制hadoop.dll到C:\\Windows\\System32目录下 4.配置环境变量 4.1配置hadoop_home  4.2Path路径添加hadoop_home/bin 2.maven项目配置Hadoop 1.pom文件 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;project&nbsp;xmlns=\"http://maven.apache.org/POM/4.0.0\" &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" &nbsp....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678937003866.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-17060812201894802840-cHOlGII.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"}]}