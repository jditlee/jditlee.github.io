{"relevantArticles":[{"articleTitle":"【bigdata】Hbase总结","articleAbstractText":"架构原理  1）StoreFile 保存实际数据的物理文件，StoreFile 以 HFile 的形式存储在 HDFS 上。每个 Store 会有 一个或多个 StoreFile（HFile），数据在每个 StoreFile 中都是有序的。 2）MemStore 写缓存，由于 HFile 中的数据要求是有序的，所以数据是先存储在 MemStore 中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的 HFile。 3）WAL 由于数据要经 MemStore 排序后才能刷写到 HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做 Write-Ahead logfile 的文件中，然后再写入MemStore 中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。 写流程  1）Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 Region Server。 2）访问对应的 Region Server，获取 hbase:meta 表，根据读请求的 namespace:table/rowke....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678938058876.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-7141292545555375219-g4X2XzM.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【bigdata】3.idea连接虚拟机Hadoop集群","articleAbstractText":"1.windows配置Hadoop环境 1.Linux环境下解压hadoop-2.6.5的tar包 tar -zxvf hadoop-2.6.5.tar.gz，复制解压包到windows的任意目录下 2.下载winutils,copy相应版本bin目录下的winutils.exe和hadoop.dll到解压的Hadoop目录下 3.复制hadoop.dll到C:\\Windows\\System32目录下 4.配置环境变量 4.1配置hadoop_home  4.2Path路径添加hadoop_home/bin 2.maven项目配置Hadoop 1.pom文件 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;project&nbsp;xmlns=\"http://maven.apache.org/POM/4.0.0\" &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" &nbsp....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678937003866.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-17060812201894802840-cHOlGII.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【bigdata】Hadoop总结","articleAbstractText":"hadoop组成 hdfs架构 namenode：存储文件的元数据 datanode: 存储文件块数据 文件块大小，2以前64M,2之后是128M secondary namenode:监控hdfs状态的辅助后台程序，每隔一段时间获取元数据的快照 hdfs写数据流程  1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。 2）NameNode返回是否可以上传。 3）客户端请求第一个 Block上传到哪几个DataNode服务器上。 4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。 5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 6）dn1、dn2、dn3逐级应答客户端。 7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个pac....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678937719444.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-15822367608376695852-lN8cZwI.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【bigdata】Flink总结","articleAbstractText":"Flink 几个最基础的概念: Client、JobManager 和 TaskManager。Client 用来提交任务给 JobManager，JobManager 分发任务给 TaskManager 去执行，然后 TaskManager 会心跳的汇报任务状态 窗口： stream.timewindow:时间 stream.countwindow:计数 stream.window(SessionWindows.withGap(Time.minutes(5))：会话 Flink程序的基本构建块是流和转换。 一个程序的基本构成： l 获取execution environment l 加载/创建原始数据 l 指定这些数据的转化方法 l 指定计算结果的存放位置 l 触发程序执行 import&nbsp;org.apache.flink.api.common.functions.FlatMapFunction; import&nbsp;org.apache.flink.api.java.tuple.Tuple2; import&nbsp;org.apache.flink.streaming....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678938322285.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-3494035270768043777-E6F9Est.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【bigdata】4.hive安装","articleAbstractText":"hive的全部安装过程都是在master节点 安装hive 1.上传并解压 tar -zxvf apache-hive-1.2.2-bin.tar.gz -C /hive安装目录  2.配置环境 2.1 配置hive-env.sh # 跳转到hive配置文件目录 cd /hive安装目录/conf # 修改名称 mv hive-env.sh.template hive-env.sh # 编辑文件 vim hive-env.sh  添加如下内容： export JAVA_HOME=/java安装目录/jdk1.8.0_172 export HADOOP_HOME=/hadoop安装目录/hadoop-2.6.1 export HIVE_HOME=/hive安装目录/apache-hive-1.2.2-bin export HIVE_CONF_DIR=/hive安装目录/apache-hive-1.2.2-bin/conf  2.2 配置hive-site.sh vim hive-site.sh #添加如下内容 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678936506568.html","articleImg1URL":"https://b3logfile.com/file/2023/03/image-Cnwm1Gy.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"}]}