{"relevantArticles":[{"articleTitle":"【bigdata】3.idea连接虚拟机Hadoop集群","articleAbstractText":"1.windows配置Hadoop环境 1.Linux环境下解压hadoop-2.6.5的tar包 tar -zxvf hadoop-2.6.5.tar.gz，复制解压包到windows的任意目录下 2.下载winutils,copy相应版本bin目录下的winutils.exe和hadoop.dll到解压的Hadoop目录下 3.复制hadoop.dll到C:\\Windows\\System32目录下 4.配置环境变量 4.1配置hadoop_home  4.2Path路径添加hadoop_home/bin 2.maven项目配置Hadoop 1.pom文件 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;project&nbsp;xmlns=\"http://maven.apache.org/POM/4.0.0\" &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" &nbsp....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678937003866.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-17060812201894802840-cHOlGII.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【bigdata】Flink总结","articleAbstractText":"Flink 几个最基础的概念: Client、JobManager 和 TaskManager。Client 用来提交任务给 JobManager，JobManager 分发任务给 TaskManager 去执行，然后 TaskManager 会心跳的汇报任务状态 窗口： stream.timewindow:时间 stream.countwindow:计数 stream.window(SessionWindows.withGap(Time.minutes(5))：会话 Flink程序的基本构建块是流和转换。 一个程序的基本构成： l 获取execution environment l 加载/创建原始数据 l 指定这些数据的转化方法 l 指定计算结果的存放位置 l 触发程序执行 import&nbsp;org.apache.flink.api.common.functions.FlatMapFunction; import&nbsp;org.apache.flink.api.java.tuple.Tuple2; import&nbsp;org.apache.flink.streaming....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678938322285.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-3494035270768043777-E6F9Est.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【bigdata】Hbase总结","articleAbstractText":"架构原理  1）StoreFile 保存实际数据的物理文件，StoreFile 以 HFile 的形式存储在 HDFS 上。每个 Store 会有 一个或多个 StoreFile（HFile），数据在每个 StoreFile 中都是有序的。 2）MemStore 写缓存，由于 HFile 中的数据要求是有序的，所以数据是先存储在 MemStore 中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的 HFile。 3）WAL 由于数据要经 MemStore 排序后才能刷写到 HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做 Write-Ahead logfile 的文件中，然后再写入MemStore 中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。 写流程  1）Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 Region Server。 2）访问对应的 Region Server，获取 hbase:meta 表，根据读请求的 namespace:table/rowke....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678938058876.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-7141292545555375219-g4X2XzM.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【bigdata】1.hadoop集群搭建","articleAbstractText":"安装虚拟机 打开VMware-》文件-》新建虚拟机： 默认典型，下一步 选择镜像文件位置，下一步 设置用户名密码，下一步（注意，有些镜像是在安装过程中设置，我用的镜像是：CentOS-7-x86_64-DVD-1804.iso，18年版本的都能先设置用户名密码，后面全程自动安装） 设置虚拟机名称和位置，下一步 设置磁盘大小，下一步 默认配置，点击完成，然后就等待系统安装完成（其它硬件配置可以后面再修改） 虚拟机配置修改，网络连接选择NAT模式，其他配置根据自己电脑情况修改 配置ip，hostname 打开VMware-》编辑-》虚拟网络编辑器 设置子网ip：  点击NAT设置，设置网关地址（这里的网关地址就是虚拟机里面需要配置的网关地址）：  点击DHCP设置，设置IP地址的范围： 打开创建好的虚拟机，配置IP地址： vim /etc/sysconfig/network-scripts/ifcfg-ens33 -- 修改以下配置 ONBOOT=yes BOOTPROTO=static IPADDR=192.168.1.101 GATEWAY=192.168.1.2 DNS1=192.1....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678936070647.html","articleImg1URL":"https://b3logfile.com/file/2023/03/solo-fetchupload-6630022845724096305-TgQO57k.png?imageView2/2/w/1280/format/jpg/interlace/1/q/100"},{"articleTitle":"【bigdata】2.Hadoop集群模式安装","articleAbstractText":"上传Hadoop并解压 1.上传Hadoop2.6.5到master 2.解压 tar -zxvf hadoop-2.6.5.tar.gz -C /opt/module/ Hadoop配置 Hadoop配置文件位置：hadoop-2.6.5/etc/hadoop 1. hadoop-env.sh -- 修改Javahome export JAVA_HOME=/opt/module/jdk1.8.0_212  2. yarn-env.sh -- 增加Javahome export JAVA_HOME=/opt/module/jdk1.8.0_212  3. core-site.xml &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt; &lt;!-- Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this f....","articleStatus":0,"articlePermalink":"/articles/2023/03/16/1678936882691.html","articleImg1URL":"https://b3logfile.com/bing/20211110.jpg?imageView2/1/w/1280/h/720/interlace/1/q/100"}]}